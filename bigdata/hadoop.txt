最初的Hadoop包含两个组件：
    · Hadoop Distributed File System (HDFS)
    · 分布式计算引擎，该引擎支持以MapReduce作业的形式实现和运行程序

Hadoop还提供了软件基础架构，以一系列map和reduce任务的形式运行MapReduce作业。Map任务在输入数据的子集上调用map函数。在完成这些调用后，reduce任务开始在map函数所生成的中间数据上调用reduce任务，生成最终的输出。map和reduce任务彼此独立运行，这支持并行和容错的计算。
Hadoop基础架构负责处理分布式处理的所有复杂方面：并行化、调度、资源管理、机器间通信、软件和硬件故障处理等等。
实现处理数百（甚至数千）台机器上数TB数据的分布式应用程序从未像现在这么容易，甚至对于之前没有使用分布式系统的经验的开发人员也是如此。

========

经典MapReduce的局限性: 可伸缩性、资源利用、对与MapReduce不同的工作负载的支持。
